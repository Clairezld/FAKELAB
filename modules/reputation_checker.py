import json
import tldextract
import wikipediaapi
import os

class ReputationChecker:
    def __init__(self):
        # Configuration Wikip√©dia (User-Agent requis par leur politique)
        self.wiki = wikipediaapi.Wikipedia(
            user_agent='FakeLabProject/1.0 (contact@fakelab.org)',
            language='fr'
        )
        self.local_db = self._load_local_db()

    def _load_local_db(self):
        """
        Charge la base de donn√©es locale (Format simple Whitelist/Blacklist).
        """
        try:
            with open('sources.json', 'r', encoding='utf-8') as f:
                return json.load(f)
        except Exception as e:
            print(f"Erreur chargement DB : {e}")
            return {"whitelist": [], "blacklist": []}

    def get_domain(self, url):
        """Extrait le domaine principal (ex: 'lemonde.fr' depuis 'http://www.lemonde.fr/article')"""
        # Si c'est d√©j√† juste un domaine, tldextract le g√®re bien
        extracted = tldextract.extract(url)
        if extracted.suffix:
            return f"{extracted.domain}.{extracted.suffix}"
        return extracted.domain # Cas localhost ou sans suffixe

    def check_source(self, url):
        """
        Analyse la r√©putation du domaine.
        Retourne : (Score, Status, M√©thode, D√©tails)
        Score est maintenant entre 0 et 1 (automatique selon la liste).
        """
        domain = self.get_domain(url)
        print(f"üîé Analyse du domaine : {domain}")

    def check_source(self, url):
        """
        Analyse la r√©putation du domaine.
        Combine Base Locale ET Wikip√©dia pour un r√©sultat robuste.
        """
        domain = self.get_domain(url)
        print(f"üîé Analyse du domaine : {domain}")

        # 1. V√©rification Base Locale
        local_score = None
        local_status = None
        local_comment = ""
        
        if domain in self.local_db.get('blacklist', []):
            local_score = 0.0
            local_status = "DANGEREUX"
            local_comment = "Liste Noire"
        elif domain in self.local_db.get('whitelist', []):
            local_score = 1.0
            local_status = "FIABLE"
            local_comment = "Liste Blanche"

        # 2. V√©rification Wikip√©dia (Toujours ex√©cut√©e pour cross-check en arri√®re-plan)
        print("   ...Interrogation de Wikip√©dia (Analyse crois√©e)...")
        wiki_score, wiki_status, wiki_source, wiki_details = self._check_wikipedia(domain)

        # 3. Consolidation des r√©sultats
        if local_score is not None:
            # Si pr√©sent localement, le score local prime (c'est notre v√©rit√© terrain)
            # Mais on enrichit les d√©tails avec les infos Wiki
            final_details = (f"üìç [LOCAL] {local_comment}. "
                             f"üìö [WIKIPEDIA] {wiki_details} (Statut Wiki: {wiki_status})")
            
            return local_score, local_status, "Hybride (Locale + Wiki)", final_details

        # Sinon, on se base enti√®rement sur Wikip√©dia
        return wiki_score, wiki_status, wiki_source, wiki_details

    def _check_wikipedia(self, domain):
        """
        Cherche le site sur Wikip√©dia et calcule un score intelligent bas√© sur le vocabulaire utilis√©.
        """
        # 1. Heuristique sur le nom de domaine (Bonus/Malus imm√©diats)
        if ".gouv." in domain or ".gov" in domain:
            return 1.0, "OFFICIEL", "Heuristique TLD", "Extension gouvernementale (.gouv/.gov) d√©tect√©e."
        if ".edu" in domain:
            return 0.95, "ACAD√âMIQUE", "Heuristique TLD", "Site universitaire ou √©ducatif."

        # 2. Recherche Wikip√©dia
        search_terms = [domain, domain.split('.')[0]]
        page = None
        for term in search_terms:
            page = self.wiki.page(term)
            if page.exists():
                break
        
        if not page or not page.exists():
            return 0.5, "INCONNU", "Non trouv√©", "Aucune donn√©e sur ce site (Score neutre 0.5)."

        summary = page.summary.lower()
        
        # --- LOGIQUE DE SCORING INTELLIGENT ---
        
        # Cat√©gorie 1 : Tr√®s Fiable (Agences, Service Public)
        if any(w in summary for w in ["agence de presse", "service public", "√©tablissement public"]):
            return 1.0, "TR√àS FIABLE", "Analyse Wikip√©dia", "Source institutionnelle ou agence de r√©f√©rence."

        # Cat√©gorie 2 : Presse √âtablie
        if any(w in summary for w in ["journal quotidien", "presse quotidienne", "journal d'information", "m√©dia d'information"]):
            return 0.9, "FIABLE", "Analyse Wikip√©dia", "M√©dia de presse reconnu."

        # Cat√©gorie 3 : Presse Magazine / Web (Neutre positif)
        if any(w in summary for w in ["hebdomadaire", "magazine", "site web d'information", "pure player"]):
            return 0.8, "G√âN√âRALEMENT FIABLE", "Analyse Wikip√©dia", "M√©dia d'information standard."

        # Cat√©gorie 4 : Satire (Faux mais "honn√™te")
        if any(w in summary for w in ["satirique", "parodique", "pastiche", "humoristique"]):
            return 0.2, "SATIRIQUE", "Analyse Wikip√©dia", "Site √† but humoristique, ne pas prendre au premier degr√©."

        # Cat√©gorie 5 : D√©sinformation / Douteux (Toxique)
        if any(w in summary for w in ["fake news", "fausses nouvelles", "d√©sinformation", "complotiste", "extr√™me droite", "propagande", "conspiration"]):
            return 0.0, "DANGEREUX", "Analyse Wikip√©dia [ALERTE]", "Site associ√© √† de la d√©sinformation ou th√©ories du complot."

        # Par d√©faut
        return 0.5, "NEUTRE", "Wikip√©dia (Ind√©cis)", "Page trouv√©e mais sans marqueur fort de fiabilit√© ou danger."

def main():
    print("=======================================================")
    print("      FAKELAB - V√©rificateur de R√©putation (Source)    ")
    print("=======================================================")
    
    checker = ReputationChecker()
    
    while True:
        url = input("\nEntrez une URL √† v√©rifier (ex: lemonde.fr) [q pour quitter] : ").strip()
        if url.lower() in ['q', 'quit']: break
        
        if not url: continue
        if "." not in url: 
            print("URL invalide (manque l'extension .fr, .com...)")
            continue

        score, status, source, details = checker.check_source(url)
        
        print(f"\nR√âSULTAT pour '{url}' :")
        print(f"üéØ Score   : {score}/100")
        print(f"üö¶ Statut  : {status}")
        print(f"‚ÑπÔ∏è  Source  : {source}")
        print(f"üìù D√©tails : {details}")

if __name__ == "__main__":
    main()
